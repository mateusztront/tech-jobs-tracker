name: Daily Job Scraper

on:
  # Run daily at 6:00 AM UTC (8:00 AM Poland time)
  schedule:
    - cron: '0 6 * * *'

  # Allow manual triggering from GitHub Actions tab
  workflow_dispatch:
    inputs:
      max_jobs:
        description: 'Maximum number of jobs to scrape'
        required: false
        default: '1000'

jobs:
  scrape-and-update:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Initialize database if needed
        run: |
          if [ ! -f data/jobs.db ]; then
            echo "Database not found, initializing..."
            python scripts/init_database.py
          else
            echo "Database exists, continuing..."
          fi

      - name: Run ETL pipeline
        id: etl
        run: |
          echo "Starting ETL pipeline..."
          python scripts/run_etl.py
        env:
          PYTHONUNBUFFERED: 1
        continue-on-error: true

      - name: Check ETL status
        id: check_status
        run: |
          if [ ${{ steps.etl.outcome }} == "success" ]; then
            echo "status=success" >> $GITHUB_OUTPUT
            echo "‚úì ETL pipeline completed successfully"
          else
            echo "status=failed" >> $GITHUB_OUTPUT
            echo "‚úó ETL pipeline failed"
            exit 0
          fi

      - name: Check for database changes
        id: check_changes
        run: |
          if git diff --quiet data/jobs.db; then
            echo "changed=false" >> $GITHUB_OUTPUT
            echo "No changes to database"
          else
            echo "changed=true" >> $GITHUB_OUTPUT
            echo "Database has been updated"
          fi

      - name: Commit and push database
        if: steps.check_changes.outputs.changed == 'true' && steps.check_status.outputs.status == 'success'
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add data/jobs.db
          git add logs/*.log

          # Create commit message with stats
          COMMIT_MSG="chore: update job data - $(date +'%Y-%m-%d')

          Automated daily scrape completed

          ü§ñ Generated by GitHub Actions"

          git commit -m "$COMMIT_MSG"
          git push

      - name: Upload logs as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.run_number }}
          path: logs/
          retention-days: 30

      - name: Generate summary
        if: always()
        run: |
          echo "## üìä Scraper Run Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Date:** $(date +'%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "**Status:** ${{ steps.check_status.outputs.status }}" >> $GITHUB_STEP_SUMMARY
          echo "**Database Updated:** ${{ steps.check_changes.outputs.changed }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f logs/etl_*.log ]; then
            LATEST_LOG=$(ls -t logs/etl_*.log | head -1)
            echo "### Log Excerpt:" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
            tail -20 "$LATEST_LOG" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Create issue on failure
        if: steps.check_status.outputs.status == 'failed'
        uses: actions/github-script@v7
        with:
          script: |
            const date = new Date().toISOString().split('T')[0];
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `‚ùå Scraper failed on ${date}`,
              body: `The daily scraper workflow failed on ${date}.\n\n` +
                    `**Run:** ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}\n\n` +
                    `Please check the logs for details.`,
              labels: ['automation', 'bug', 'scraper-failure']
            });
