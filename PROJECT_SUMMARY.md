# NoFluffJobs IT Market Dashboard - Project Summary

## ğŸ¯ Project Overview

A complete data analytics platform that automatically scrapes NoFluffJobs daily to track IT job market trends in Poland. Features real-time dashboards, historical trend analysis, and automated data collection.

**Built by:** Data Engineer with 4 years of experience in Python and SQL
**Purpose:** Portfolio project demonstrating end-to-end data engineering skills
**Status:** âœ… Fully Functional (Phases 1-5 Complete)

## ğŸ“Š Key Features

- **Automated Daily Scraping**: GitHub Actions scrapes NoFluffJobs at 6 AM UTC daily
- **Historical Tracking**: Maintains snapshots to analyze trends over time
- **Interactive Dashboard**: Streamlit-based with 19 different visualizations
- **Comprehensive ETL**: Extract â†’ Transform â†’ Load pipeline with data quality checks
- **Real-time Monitoring**: Auto-creates GitHub issues on scraper failures

### Analytics Capabilities

1. **Salary Analysis**
   - Distribution across all jobs
   - Comparison by technology (Top 20)
   - Trends over time
   - Breakdown by seniority level (Junior/Mid/Senior)
   - Comparison by location type (Remote/Office/Hybrid)

2. **Technology Trends**
   - Most in-demand technologies
   - Popularity trends over time
   - Category distribution (Language/Framework/Database/Cloud/Tool)
   - Demand heatmap
   - Technology co-occurrence

3. **Geographic Insights**
   - Jobs by city (Top 15)
   - Regional distribution
   - Remote work percentage trend
   - Location type distribution

4. **Skills Intelligence**
   - Top paying technologies
   - Skills by category
   - Demand patterns

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GitHub Actions (Automation)               â”‚
â”‚         Daily at 6:00 AM UTC â”‚ Manual Trigger Available      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Web Scraper (Phase 2)                      â”‚
â”‚  â€¢ Rate Limiter (2-5s delays)  â€¢ Circuit Breaker            â”‚
â”‚  â€¢ HTML Parser                  â€¢ Error Handling             â”‚
â”‚  â€¢ Pagination Handler           â€¢ Retry Logic (3x)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ETL Pipeline (Phase 3)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ Extractor â”‚ â†’ â”‚Transformer â”‚ â†’ â”‚  Loader  â”‚             â”‚
â”‚  â”‚ Validate  â”‚   â”‚ Normalize  â”‚   â”‚  Upsert  â”‚             â”‚
â”‚  â”‚  Clean    â”‚   â”‚Categorize  â”‚   â”‚  Track   â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 SQLite Database (Phase 1)                    â”‚
â”‚  â€¢ 7 Tables (job_postings, job_snapshots, salaries, etc.)   â”‚
â”‚  â€¢ 10 Indexes for performance                               â”‚
â”‚  â€¢ Foreign key constraints                                  â”‚
â”‚  â€¢ Historical snapshots for trend analysis                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Streamlit Dashboard (Phase 4)                    â”‚
â”‚  â€¢ 5 Interactive Tabs  â€¢ 19 Visualizations                  â”‚
â”‚  â€¢ KPI Metrics         â€¢ Date Filtering                     â”‚
â”‚  â€¢ Data Caching (1hr)  â€¢ Responsive Layout                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
nofluffjobs-dashboard/
â”œâ”€â”€ .github/workflows/          # GitHub Actions automation
â”‚   â”œâ”€â”€ daily_scrape.yml       # Daily scraping workflow
â”‚   â””â”€â”€ update_stats.yml       # Statistics updater
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper/               # Web scraping components
â”‚   â”‚   â”œâ”€â”€ nofluff_scraper.py # Main scraper (2-phase: URLs â†’ Details)
â”‚   â”‚   â”œâ”€â”€ parser.py          # HTML parsing logic
â”‚   â”‚   â””â”€â”€ rate_limiter.py    # Rate limiting + circuit breaker
â”‚   â”‚
â”‚   â”œâ”€â”€ database/              # Database layer
â”‚   â”‚   â”œâ”€â”€ models.py          # Schema definitions (7 tables)
â”‚   â”‚   â””â”€â”€ db_manager.py      # Connection management, transactions
â”‚   â”‚
â”‚   â”œâ”€â”€ etl/                   # ETL pipeline
â”‚   â”‚   â”œâ”€â”€ extractor.py       # Data extraction & validation
â”‚   â”‚   â”œâ”€â”€ transformer.py     # Normalization & categorization
â”‚   â”‚   â””â”€â”€ loader.py          # Database loading with upserts
â”‚   â”‚
â”‚   â””â”€â”€ dashboard/             # Streamlit dashboard
â”‚       â”œâ”€â”€ app.py             # Main dashboard application
â”‚       â”œâ”€â”€ components/        # Visualization components
â”‚       â”‚   â”œâ”€â”€ salary_charts.py    (6 charts)
â”‚       â”‚   â”œâ”€â”€ tech_trends.py      (6 charts)
â”‚       â”‚   â””â”€â”€ geo_charts.py       (7 charts)
â”‚       â””â”€â”€ utils/
â”‚           â””â”€â”€ data_loader.py # Optimized database queries
â”‚
â”œâ”€â”€ config/                    # Configuration files
â”‚   â”œâ”€â”€ config.yaml           # General settings
â”‚   â””â”€â”€ scraper_config.yaml   # Scraper parameters
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ jobs.db               # SQLite database (committed by Actions)
â”‚
â”œâ”€â”€ logs/                     # Application logs
â”œâ”€â”€ scripts/                  # Utility scripts
â”‚   â”œâ”€â”€ init_database.py     # Initialize database
â”‚   â”œâ”€â”€ run_scraper.py       # Manual scraper execution
â”‚   â”œâ”€â”€ run_etl.py           # Complete ETL pipeline
â”‚   â”œâ”€â”€ populate_sample_data.py  # Sample data generator
â”‚   â”œâ”€â”€ test_database.py     # Database tests
â”‚   â”œâ”€â”€ test_scraper.py      # Scraper tests
â”‚   â””â”€â”€ test_etl.py          # ETL tests
â”‚
â”œâ”€â”€ tests/                    # Unit tests
â”œâ”€â”€ README.md                 # Main documentation
â”œâ”€â”€ DASHBOARD_README.md       # Dashboard usage guide
â”œâ”€â”€ GITHUB_ACTIONS_SETUP.md   # Automation setup guide
â””â”€â”€ requirements.txt          # Python dependencies
```

## ğŸ› ï¸ Technology Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Language** | Python 3.11+ | Core development |
| **Database** | SQLite | Data storage with ACID compliance |
| **Web Scraping** | Requests + BeautifulSoup | HTML parsing and extraction |
| **Dashboard** | Streamlit + Plotly | Interactive visualizations |
| **Data Processing** | Pandas | Data manipulation and analysis |
| **Automation** | GitHub Actions | Scheduled daily execution |
| **Configuration** | YAML | Settings management |
| **Testing** | Pytest | Component testing |

## ğŸ“ˆ Database Schema

### Core Tables (7)

1. **job_postings** - Main job entries
   - Fields: job_id (PK), title, company_name, url, first_seen_date, last_seen_date, is_active
   - Purpose: Track unique job postings and lifecycle

2. **job_snapshots** - Historical snapshots
   - Fields: job_id, snapshot_date, description, location_type, city, region, seniority_level
   - Purpose: Daily snapshots for trend analysis

3. **salaries** - Compensation data
   - Fields: job_id, snapshot_date, salary_min, salary_max, salary_avg, currency, is_b2b
   - Purpose: Track salary ranges and types

4. **technologies** - Normalized tech list
   - Fields: id (PK), name (unique), category
   - Purpose: Categorize technologies (30+ tracked)

5. **job_technologies** - Many-to-many relationships
   - Fields: job_id, technology_id, proficiency_level, snapshot_date
   - Purpose: Link jobs to required technologies

6. **daily_metrics** - Aggregated statistics
   - Fields: metric_date, total_jobs, remote_jobs, avg_salary_pln, median_salary_pln
   - Purpose: Pre-calculated metrics for dashboard performance

7. **scrape_runs** - Execution metadata
   - Fields: run_date, jobs_found, jobs_new, jobs_updated, status, duration_seconds
   - Purpose: Monitor scraper health and performance

### Indexes (10)
- Optimized queries on job_id, snapshot_date, active status, and technology names
- Ensures fast dashboard loading even with large datasets

## ğŸ”„ Data Flow

### 1. Scraping Phase
```
NoFluffJobs Website
    â†“ (Rate-limited requests)
Raw HTML Pages
    â†“ (BeautifulSoup parsing)
Raw Job Data (JSON)
```

### 2. ETL Phase
```
Raw Job Data
    â†“ (Extractor: validate, clean)
Extracted Data
    â†“ (Transformer: normalize, categorize)
Transformed Data
    â†“ (Loader: upsert, track)
SQLite Database
```

### 3. Dashboard Phase
```
SQLite Database
    â†“ (Optimized queries)
Cached DataFrames (1hr TTL)
    â†“ (Plotly visualizations)
Interactive Dashboard
```

## ğŸš€ Key Accomplishments

### Phase 1: Foundation âœ…
- âœ… 7-table database schema with foreign keys
- âœ… Transaction-safe database manager
- âœ… Comprehensive configuration system
- âœ… Project structure following best practices

### Phase 2: Web Scraper âœ…
- âœ… Ethical scraping with 2-5s delays
- âœ… Circuit breaker pattern (prevents hammering on failures)
- âœ… Exponential backoff retry logic (3 attempts)
- âœ… Pagination handling
- âœ… 429 rate limit detection and handling

### Phase 3: ETL Pipeline âœ…
- âœ… Salary normalization: "15 000 - 20 000 PLN" â†’ {min: 15000, max: 20000, avg: 17500}
- âœ… Location standardization: "Warszawa / Zdalnie" â†’ {city: Warszawa, type: hybrid}
- âœ… Technology categorization: 30+ technologies across 5 categories
- âœ… Idempotent loading (no duplicates on re-run)
- âœ… Job expiration tracking

### Phase 4: Dashboard âœ…
- âœ… 19 interactive visualizations
- âœ… 5 analytical tabs
- âœ… KPI metrics dashboard
- âœ… Date range filtering
- âœ… Data caching for performance
- âœ… Responsive layout

### Phase 5: Automation âœ…
- âœ… Daily GitHub Actions workflow
- âœ… Auto-commit database updates
- âœ… Failure monitoring (auto-creates issues)
- âœ… Artifact upload (logs retention)
- âœ… Statistics updater workflow

## ğŸ“Š Sample Results

With sample data (100 jobs):
- **Technologies Tracked**: 30 unique
- **Cities**: 8 Polish cities
- **Salary Range**: 6,000 - 28,000 PLN
- **Location Types**: Remote (33%), Office (33%), Hybrid (33%)
- **Seniority Levels**: Junior, Mid, Senior distribution

## ğŸ“ Learning Outcomes

This project demonstrates proficiency in:

1. **Data Engineering**
   - ETL pipeline design and implementation
   - Database schema design with normalization
   - Data quality and validation
   - Historical data tracking

2. **Web Scraping**
   - Ethical scraping practices
   - Rate limiting and retry logic
   - HTML parsing and extraction
   - Error handling and resilience

3. **Data Visualization**
   - Interactive dashboard design
   - Multiple visualization types
   - User experience considerations
   - Performance optimization

4. **Automation & DevOps**
   - GitHub Actions workflows
   - Scheduled task execution
   - Monitoring and alerting
   - CI/CD concepts

5. **Software Engineering**
   - Modular architecture
   - Code organization
   - Testing practices
   - Documentation

## ğŸ”® Future Enhancements (Optional)

1. **Advanced Analytics**
   - Salary prediction ML model
   - Job recommendation system
   - Skill gap analysis
   - Market saturation indicators

2. **Extended Coverage**
   - Multiple job boards (JustJoin.IT, Pracuj.pl)
   - International markets
   - Comparison analysis

3. **Real-time Features**
   - Email alerts for matching jobs
   - Webhook notifications
   - Real-time dashboard updates

4. **Scalability**
   - PostgreSQL migration (for larger datasets)
   - Redis caching
   - Horizontal scaling

5. **Deployment**
   - Streamlit Cloud deployment
   - Docker containerization
   - API for programmatic access

## ğŸ“ Documentation

| Document | Purpose |
|----------|---------|
| README.md | Project overview and quick start |
| DASHBOARD_README.md | Dashboard usage and features |
| GITHUB_ACTIONS_SETUP.md | Automation setup guide |
| PROJECT_SUMMARY.md | Comprehensive project overview (this file) |

## ğŸ† Project Highlights

- **Fully Functional**: All 5 phases implemented and tested
- **Production-Ready**: Error handling, logging, monitoring
- **Well-Documented**: 4 comprehensive documentation files
- **Tested**: Component tests for database, scraper, and ETL
- **Automated**: Hands-free daily operation via GitHub Actions
- **Scalable**: Designed to handle growing datasets
- **Ethical**: Respects website terms and implements rate limiting

## ğŸ“ Support & Contributing

This is a personal portfolio project. Feel free to:
- Fork and adapt for your own use
- Reference in your own projects
- Learn from the implementation

## âš–ï¸ License

MIT License - See LICENSE file for details

## ğŸ™ Acknowledgments

- **NoFluffJobs** - Data source (educational use only)
- **Streamlit** - Dashboard framework
- **Plotly** - Visualization library
- **GitHub Actions** - Automation platform

---

**Project Status**: âœ… Complete (5/6 phases)
**Last Updated**: 2026-01-06
**Built with**: â¤ï¸ and lots of â˜•
